{
  "hash": "94441f4b3dde275fb16df5f6445c684a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Let's cube those geoms!\"\ndescription: |\n  Vector data cubes for multidimensional data.\ndate: 14/10/2024\ndate-modified: last-modified\n# image: figs/thumbnail.png\ncategories:\n  - r-spatial\n  - rstats\nbibliography: https://api.citedrive.com/bib/618133a2-4856-443b-b778-a72d316eb599/references.bib?x=eyJpZCI6ICI2MTgxMzNhMi00ODU2LTQ0M2ItYjc3OC1hNzJkMzE2ZWI1OTkiLCAidXNlciI6ICIyNTY4IiwgInNpZ25hdHVyZSI6ICI4MWExNDU3ODE3NWUxMzFiZDI1NDliOTg2NmQzMWJjNzU4OWI4YjI3MDUxOTU5NDg3MDM5MzFlYmQwMDY0OThhIn0=/bibliography.bib\ndraft: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# BEGIN seminar\n\nIf you are here, it is very likely you attended the [BEGIN seminar from the University of St. Andrews](https://begin.wp.st-andrews.ac.uk/activities-events/) today. Thank you for stopping by! ğŸ’\n\nI was invited by Dr. Fernando Benitez (thank you so much for considering me!) to give a talk on a topic I am working on, so I decided to [talk about vector data cubes](https://begin.wp.st-andrews.ac.uk/2024/09/26/begin-seminar-dr-lorena-cristina-abad-crespo/) ğŸ˜Š.\nThis post compliments the talk by writing out what I discussed during the seminar and by providing the code to reproduce some of the work I showed. \n\nHere are the slides of my talk today:\n\n<iframe width=\"620\" height=\"390\" src=\"https://loreabad6.github.io/slides/begin-vdcs.html\" frameborder=\"0\">\n<p>Your browser does not support iframes.</p>\n</iframe>\n\nAnd over here I will load some of the libraries I use for the next sections.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cubble)\nlibrary(glue)\nlibrary(here)\nif(!require(post)) remotes::install_github(\"loreabad6/post\")\nlibrary(post)\nlibrary(sf)\nlibrary(stars)\nlibrary(tidyverse)\nlibrary(units)\n```\n:::\n\n\n\n\n# Data cubes\n\nData cubes are ideal data structures for multidimensional data. Think of data with spatiotemporal properties such as satellite image time series (SITS). If you work with SITS you might already be familiar with data cubes, as Earth system data cubes have received quite some attention lately [@Mahecha2020].\n\n::: column-margin\n![Earth system data cube \\| Source: @Mahecha2020](https://esd.copernicus.org/articles/11/201/2020/esd-11-201-2020-f03-web.png)\n:::\n\nTools and initiatives like [Google Earth Engine](https://earthengine.google.com/), [OpenEO](https://openeo.org/) and [Digital Earth Africa](https://www.digitalearthafrica.org/) are using data cubes as their core for SITS analysis.\n\nOrganizing SITS in data cubes is intuitive and logical given that satellite images have consistent X/Y coordinates per pixel, and any reacquisition over time keeps the same X/Y coordinates. Here we already identify three dimensions: `X`, `Y` and `time`. Satellite images also have bands and we can then consider those as a fourth dimension.\n\n![4D data cube \\| Source: @Pebesma_2023](figs/fig-cube4d-1.png)\n\n::: column-margin\nI wrote a post almost a year ago on how to create [on-demand data cubes with STAC with R and Python](https://loreabad6.github.io/posts/2023-10-17-stac-across-languages/).\n:::\n\n# Vector data cubes\n\nTo quote @Pebesma_2023 at the very start of the Data Cubes Chapter:\n\n> â€œData cubes arise naturally when we observe properties of a set of geometries repeatedly over timeâ€.\n\nClearly satellite images for this description, being X and Y said â€œgeometryâ€. But how other type of data like meteorological station data? They are also located in a X and Y location on Earth, represented by a `POINT` geometry. Stations collect data at constant time intervals, giving them a temporal dimension. Further, they can collect data on different type of phenomena, such as temperature, humidity, etc. So in essence, station data is multidimensional data that can be represented as a data cube. In this case, a *vector data cube* (VDCs).\n\n![Vector data cube \\| Source: @Pebesma_2023](figs/fig-aqdc-1.png)\n\n::: column-margin\nIf you are familiar with databases, you will find that the concept of a vector data cube aligns greatly with an [OLAP cube](https://en.wikipedia.org/wiki/OLAP_cube) model.\n:::\n\nAnother â€œnaturally occurringâ€ VDC case is when data from a raster or Earth system data cube is aggregated onto vector geometries. This vector geometries can be for example, political boundaries. The [`{stars}`](https://r-spatial.github.io/stars/index.html) package has a great example of aggregation of precipitation data over the North Carolina counties [right here](https://r-spatial.github.io/stars/index.html#raster-and-vector-time-series-analysis-example). This is a sneak peek:\n\n\n\n::: {.cell .column-page layout-ncol=\"2\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load precipitation data\nprecipitation = system.file(\"nc/test_stageiv_xyt.nc\", package = \"stars\") |>\n    read_stars()\n# Load North Carolina data\nnc = system.file(\"gpkg/nc.gpkg\", package = \"sf\") |> \n  read_sf() |> \n  # Transform CRS to match precipitation data cube\n  st_transform(st_crs(precipitation))\n# Aggregate precipitation data over NC counties\nprecipitation_nc = aggregate(precipitation, by = nc, FUN = max)\n# Plot raster data cube\nggplot() +\n  geom_stars(data = precipitation[,,,1:9]) +\n  scale_fill_viridis_c(\"Precipitation [kg/m^2]\") +\n  geom_sf(data = nc, fill = NA, color = \"red\") +\n  facet_wrap(\"time\", ncol = 3) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Precipitation raster data cube](index_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Plot vector data cube\nggplot() +\n  geom_stars(\n    data = precipitation_nc[,,1:9]\n  ) +\n  geom_sf(data = nc, fill = NA, color = \"white\") +\n  scale_fill_viridis_c(\n    \"Precipitation [kg/m^2]\",\n    na.value = \"grey85\"\n  ) +\n  facet_wrap(~time, nrow = 3) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Precipitation vector data cube aggregated by NC county](index_files/figure-html/unnamed-chunk-2-2.png){width=768}\n:::\n:::\n\n\n\n# Exploring VDCs with R\n\nIf you do a search for â€œvector data cubes Râ€, it is very likely that your first result is the `{stars}` package. As its name says, `{stars}` supports â€œspatiotemporal **arrays**, rasters, and vector data cubesâ€.\n\n::: column-margin\n`{stars}` pioneered vector data cube implementations in current spatial data science languages. Further efforts in Python with the package [`{xvec}`](https://xvec.readthedocs.io/en/stable/) are inspired by initial development in `{stars}`.\n:::\n\n`{stars}` takes advantage of base Râ€™s support for matrices and arrays to implement common data cube operations such as slicing (filtering), reducing (aggregating), and applying functions to dimensions.\n\nTypically, data cubes are represented as multidimensional arrays given their capacity to support exactly that, multiple dimensions.\nHowever, when only looking at spatio-temporal data, I also use the [`{cubble}`](https://huizezhang-sherry.github.io/cubble/) package that implements \"a vector spatiotemporal data structure for data analysis\" [@Zhang_2022].\n\n`{stars}` and `{cubble}` interact very well when the data cube consists of space and time dimensions. \n\nTo showcase this interaction I will make use of some data from the [openSenseMap](https://opensensemap.org/) project and build what I call array and tabular data cubes.\n\n::: column-margin\n[openSenseMap.org](https://opensensemap.org/) has the vision to become *\"the Wikipedia for environmental data in order to awaken and promote education, environmental and climate protection, enthusiasm for STEM, citizen science, open data and open source.\"*\n\nYou can [donate to their cause here](https://opensensemap.org/donate).\n:::\n\n## Fetching the data\n\nThe openSenseMap project provides an API to query the information from the sensors shared on their website. \n\nI queried the API to obtain some data from sensors close to the city where I am based, Salzburg.\nYou can unfold the code below to see how I queried the API and pre-processed the data so that you know how to do this for other areas.\nHowever, I saved the result of the querying and pre-processing to avoid exhausting the API.\nThe data is copyrighted as Â© senseBox 2014 - 2020.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n## parameters\nbboxsbg = glue_collapse(c(12,47.5,13.5,47.9), sep = \",\")\nfrom = \"2024-10-01T00:00:00Z\"\nto = \"2024-10-03T00:00:00Z\"\nop = \"arithmeticMean\"\nwin = \"1h\"\nformat = \"tidy\"\ncols = glue_collapse(c(\"boxId\",\"boxName\",\"lat\",\"lon\",\"phenomenon\",\"unit\"), sep = \",\")\n## api query\nquery = function(phenomenon) {\n  glue(\"https://api.opensensemap.org/statistics/descriptive?bbox={bboxsbg}&from-date={from}&to-date={to}&phenomenon={phenomenon}&operation={op}&columns={cols}&window={win}&format={format}\")\n}\n## fetch data\ntemperature = read_csv(query(\"Temperatur\"))\npm10 = read_csv(query(\"PM10\"))\npm25 = read_csv(query(\"PM2.5\"))\n## check if data is complete, if not, complete cases\ndim(temperature)\ndim(pm10)\ndim(pm25)\ntemperature_comp = temperature |> \n  complete(\n    nesting(boxName, lon, lat), time_start,\n    nesting(phenomenon, unit)\n)\n# merge data into one file and complete cases for all stations\nopensense = bind_rows(temperature_comp, pm10, pm25) |> \n  select(-sensorId, -boxId) |> \n  complete(\n    nesting(boxName, lon, lat, time_start),\n    nesting(phenomenon, unit)\n  ) |> \n  st_as_sf(coords = c(\"lon\", \"lat\")) |> \n  st_set_crs(4326)\nwrite_sf(\"data/opensensemap.gpkg\")\n```\n:::\n\n\n\nTo read in the saved data, we can use the `{sf}` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(opensense = read_sf(\"data/opensensemap.gpkg\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 1050 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 12.18844 ymin: 47.62055 xmax: 13.12862 ymax: 47.8726\nGeodetic CRS:  WGS 84\n# A tibble: 1,050 Ã— 6\n   boxName   time_start          phenomenon unit  arithmeticMean_1h\n   <chr>     <dttm>              <chr>      <chr>             <dbl>\n 1 1188BRD16 2024-10-01 02:00:00 PM10       Âµg/mÂ³             1.35 \n 2 1188BRD16 2024-10-01 02:00:00 PM2.5      Âµg/mÂ³             0.754\n 3 1188BRD16 2024-10-01 02:00:00 Temperatur Â°C               NA    \n 4 1188BRD16 2024-10-01 03:00:00 PM10       Âµg/mÂ³             1.60 \n 5 1188BRD16 2024-10-01 03:00:00 PM2.5      Âµg/mÂ³             0.707\n 6 1188BRD16 2024-10-01 03:00:00 Temperatur Â°C               NA    \n 7 1188BRD16 2024-10-01 04:00:00 PM10       Âµg/mÂ³             2.05 \n 8 1188BRD16 2024-10-01 04:00:00 PM2.5      Âµg/mÂ³             0.885\n 9 1188BRD16 2024-10-01 04:00:00 Temperatur Â°C               NA    \n10 1188BRD16 2024-10-01 05:00:00 PM10       Âµg/mÂ³             1.70 \n# â„¹ 1,040 more rows\n# â„¹ 1 more variable: geom <POINT [Â°]>\n```\n\n\n:::\n\n```{.r .cell-code}\n# (opensense = read_sf(\"posts/2024-10-14-lets-cube-those-geoms/data/opensensemap.gpkg\"))\n```\n:::\n\n\n\nAs you can notice, the data has several data duplicates, since it is presented in a long format. \nIn this way each row contains the station name (`boxName`), the timestamp (`time_start`) and the parameter measured (`phenomenon`). \nEach station also has a `POINT` geometry and each measurement has its corresponding unit. \n\nThe measured values (`arithmeticMean_1h`) are the only changing variables. \nThis value, as can be seen on the query code, is the arithmetic mean per hour of the observations for temperature, PM2.5 and PM10 for the sensors in the bounding box given.\nNot every sensor measures all the values and therefore there are `NA`s within the data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(opensense$arithmeticMean_1h))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 301\n```\n\n\n:::\n:::\n\n\n\nThat is a lot of empty data!\nThe reason to introduce so many NAs is that to create an array, we need that every dimension has an equal number of entries.\n\n## Creating a VDC with `{stars}`\n\nAlthough creating a raster data cube from SITS data is pretty straightforward in `{stars}`, creating a VDC requires a bit more pre-processing.\n\nHowever, when creating a VDC from scratch we get to understand better how [`stars` objects are structured](https://r-spatial.github.io/stars/articles/stars4.html).\n\n> \"`{stars}` provides infrastructure for *data cubes*, array data with labeled dimensions\".\n\nSo, we start with the *dimensions*.\n\n> \"A `dimensions` object is a named list of dimension elements, each describing the semantics a dimension of the data arrays (space, time, type etc).\"\n\nFirst, we identify the dimensions in the `opensense` object.\nWe have a geometry (`geom`) that identifies and locates each station in space and we have a timestamp (`time_start`) that marks the temporal dimension.\nThe `point` parameter indicates if the dimension has point support (`point = TRUE` in the case of our `geom`) or if it is a period (`point = FALSE` in the case of our `time_start`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(d = st_dimensions(\n  geom = st_sfc(unique(opensense$geom), crs = st_crs(opensense)),\n  time = unique(opensense$time_start),\n  point = c(TRUE, FALSE)\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     from to                   offset   delta  refsys point\ngeom    1  7                       NA      NA  WGS 84  TRUE\ntime    1 50 2024-10-01 02:00:00 CEST 1 hours POSIXct FALSE\n                                                      values\ngeom POINT (12.91207 47.71819),...,POINT (12.45901 47.73125)\ntime                                                    NULL\n```\n\n\n:::\n:::\n\n\n\nTake a moment to check the output of this `dimensions` object.\nWe have established the semantics of the data array dimensions. \nWith this we guarantee that there are 7 stations and 50 timestamps. \nThe `geom` dimension, that contains the geometries of the 7 stations have a CRS (WGS84), have point support and we can see a glimpse of the `POINT` geometries under the values column.\n\nThe `time` dimension on the other hand starts with index 1 and finishes at index 50.\nThe offset indicated the first time index, while the delta contains the information on the frequency of the temporal data, in this case, 1 hour.\nWe set this as a period, since it is an aggregation of all the values obtained in that hour.\nThe values column is empty since the offset and delta can save all the needed information. \nDon't worry your data is still there!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_get_dimension_values(d, \"geom\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeometry set for 7 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 12.18844 ymin: 47.62055 xmax: 13.12862 ymax: 47.8726\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOINT (12.91207 47.71819)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOINT (12.18844 47.62055)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOINT (12.64558 47.86673)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOINT (13.03966 47.82361)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPOINT (13.12862 47.65276)\n```\n\n\n:::\n\n```{.r .cell-code}\nst_get_dimension_values(d, \"time\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"2024-10-01 02:00:00 CEST\" \"2024-10-01 03:00:00 CEST\"\n [3] \"2024-10-01 04:00:00 CEST\" \"2024-10-01 05:00:00 CEST\"\n [5] \"2024-10-01 06:00:00 CEST\" \"2024-10-01 07:00:00 CEST\"\n [7] \"2024-10-01 08:00:00 CEST\" \"2024-10-01 09:00:00 CEST\"\n [9] \"2024-10-01 10:00:00 CEST\" \"2024-10-01 11:00:00 CEST\"\n[11] \"2024-10-01 12:00:00 CEST\" \"2024-10-01 13:00:00 CEST\"\n[13] \"2024-10-01 14:00:00 CEST\" \"2024-10-01 15:00:00 CEST\"\n[15] \"2024-10-01 16:00:00 CEST\" \"2024-10-01 17:00:00 CEST\"\n[17] \"2024-10-01 18:00:00 CEST\" \"2024-10-01 19:00:00 CEST\"\n[19] \"2024-10-01 20:00:00 CEST\" \"2024-10-01 21:00:00 CEST\"\n[21] \"2024-10-01 22:00:00 CEST\" \"2024-10-01 23:00:00 CEST\"\n[23] \"2024-10-02 00:00:00 CEST\" \"2024-10-02 01:00:00 CEST\"\n[25] \"2024-10-02 02:00:00 CEST\" \"2024-10-02 03:00:00 CEST\"\n[27] \"2024-10-02 04:00:00 CEST\" \"2024-10-02 05:00:00 CEST\"\n[29] \"2024-10-02 06:00:00 CEST\" \"2024-10-02 07:00:00 CEST\"\n[31] \"2024-10-02 08:00:00 CEST\" \"2024-10-02 09:00:00 CEST\"\n[33] \"2024-10-02 10:00:00 CEST\" \"2024-10-02 11:00:00 CEST\"\n[35] \"2024-10-02 12:00:00 CEST\" \"2024-10-02 13:00:00 CEST\"\n[37] \"2024-10-02 14:00:00 CEST\" \"2024-10-02 15:00:00 CEST\"\n[39] \"2024-10-02 16:00:00 CEST\" \"2024-10-02 17:00:00 CEST\"\n[41] \"2024-10-02 18:00:00 CEST\" \"2024-10-02 19:00:00 CEST\"\n[43] \"2024-10-02 20:00:00 CEST\" \"2024-10-02 21:00:00 CEST\"\n[45] \"2024-10-02 22:00:00 CEST\" \"2024-10-02 23:00:00 CEST\"\n[47] \"2024-10-03 00:00:00 CEST\" \"2024-10-03 01:00:00 CEST\"\n[49] \"2024-10-03 02:00:00 CEST\" \"2024-10-03 03:00:00 CEST\"\n```\n\n\n:::\n:::\n\n\n\nNow, let's focus on the *array data*. \n\n> \"`stars` objects consist of a (possibly empty) named list of arrays, each having named dimensions (`dim`) attribute\".\n\nTo create an array in R we need to establish the maximal indices in each dimension. \nAs we did above when building the `dimensions` object, we create now a vector with the dimensions in which we will arrange the array data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndims = c(\n  length(unique(opensense$boxName)),\n  length(unique(opensense$time_start))\n)\n```\n:::\n\n\n\nThe order in which the data is assigned to each element in an array is important. \nNormally, arrays are assigned in a per column basis. \nFor this, we order our data first by the last set dimension (time) and then by the first one (station geom).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopensense = opensense[order(opensense[[\"time_start\"]],opensense[[\"boxName\"]]), ]\n```\n:::\n\n\n\nAnd now we create the arrays to pass onto the `stars` object during creation. \nWe create one array per parameter.\nFor that we filter the data to the corresponding `phenomenon`.\nThen we extract the measured values and pass them onto the `array()` function, assigning the `dims` object created above. \nFinally, we pass the units to the array for each parameter.\n\n::: column-margin\nInstead of creating the three arrays, we can also create a third dimension for the parameters.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create temperature array\ntemp = filter(opensense, phenomenon == \"Temperatur\")\ntemp_arr = array(temp$arithmeticMean_1h, dim = dims)\nunits(temp_arr) = as_units(unique(temp$unit))\n# Create PM2.5 array\npm25 = filter(opensense, phenomenon == \"PM2.5\")\npm25_arr = array(pm25$arithmeticMean_1h, dim = dims)\nunits(pm25_arr) = as_units(unique(pm25$unit))\n# Create PM10 array\npm10 = filter(opensense, phenomenon == \"PM10\")\npm10_arr = array(pm10$arithmeticMean_1h, dim = dims)\nunits(pm10_arr) = as_units(unique(pm10$unit))\n```\n:::\n\n\n\nAnd with that, we can create our VDC using `{stars}`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(opensense_stars = st_as_stars(\n  temperature = temp_arr,\n  `PM2.5` = pm25_arr,\n  PM10 = pm10_arr,\n  d\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n                       Min.    1st Qu.    Median     Mean   3rd Qu.      Max.\ntemperature [Â°C] 6.65000000 12.7511667 15.514167 17.01426 21.291500  24.56083\nPM2.5 [Âµg/mÂ³]    0.04090909  0.6726201  1.324079 12.93712  4.611875  66.39458\nPM10 [Âµg/mÂ³]     0.04227273  1.1006522  2.612205 24.76510  8.095625 149.11708\n                 NA's\ntemperature [Â°C]  101\nPM2.5 [Âµg/mÂ³]     100\nPM10 [Âµg/mÂ³]      100\ndimension(s):\n     from to                   offset   delta  refsys point\ngeom    1  7                       NA      NA  WGS 84  TRUE\ntime    1 50 2024-10-01 02:00:00 CEST 1 hours POSIXct FALSE\n                                                      values\ngeom POINT (12.91207 47.71819),...,POINT (12.45901 47.73125)\ntime                                                    NULL\n```\n\n\n:::\n:::\n\n\n\nNote how the units for each parameter are also recognized, and how we can get a quick glimpse of the summary statistics for each one.\n\n::: column-margin\nOne thing I always find challenging is passing the arrays in the correct order. When building your own cube, make sure to double check that the original data is assigned to the correct dimensions!\n:::\n\n## Cube operations\n\nWe can take a small peek at how the PM10 data looks like for example. \nFor this we can do some **subsetting** of our data cube using `[`. \n\nThe first element corresponds to the attributes, so here we call the PM10 attribute.\nThe second element is the `geom` dimension. We want to keep all the stations, so we keep it as it is. \nFinally, the last element is the time dimension. I took a subset from the first to the ninth hour (to avoid plotting 50 hours of data but also to show you how to subset ğŸ˜‰).\n\n::: column-margin\nYou find here a glimpse of the operations you can run in a data cube with `{stars}`. Take a look at the [package vignettes](https://r-spatial.github.io/stars/articles/stars1.html#switching-attributes-to-dimensions-and-back) and the [Spatial Data Science book](https://r-spatial.org/book/06-Cubes.html#sec-dcoperations) for a more comprehensive overview.\n:::\n\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\nplot(opensense_stars[\"PM10\",,1:9], pch = 16)\n```\n\n::: {.cell-output-display}\n![PM10 hourly measurements for openSenseMap stations.](index_files/figure-html/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n\nAnother interesting operation is to convert attributes to dimensions (with `merge()`) and vice-versa (with `split()`).\nThis is helpful when specific subsetting over the dimension is desired, or if a function should be applied over set dimension. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopensense_stars |> \n  drop_units() |> \n  merge(name = \"parameter\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n                              Min. 1st Qu.  Median     Mean 3rd Qu.     Max.\ntemperature.PM2.5.PM10  0.04090909   1.213 6.16875 18.24046  20.937 149.1171\n                        NA's\ntemperature.PM2.5.PM10   301\ndimension(s):\n          from to                   offset   delta  refsys point\ngeom         1  7                       NA      NA  WGS 84  TRUE\ntime         1 50 2024-10-01 02:00:00 CEST 1 hours POSIXct FALSE\nparameter    1  3                       NA      NA      NA    NA\n                                                           values\ngeom      POINT (12.91207 47.71819),...,POINT (12.45901 47.73125)\ntime                                                         NULL\nparameter                   temperature, PM2.5      , PM10       \n```\n\n\n:::\n:::\n\n\n\nAnother strong operation in data cubes is the ability to aggregate dimensions to larger units, applying functions for this aggregation. \nHere for instance, we aggregate the data to a daily mean.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(opensense_daily = opensense_stars |> \n  aggregate(\"1 day\", mean, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n                   Min.    1st Qu.    Median     Mean   3rd Qu.      Max. NA's\ntemperature  7.44366667 12.3646661 14.953905 16.28129 21.175726  24.31992    6\nPM2.5        0.09045043  0.8438253  1.379532 12.68583  2.960180  59.37775    6\nPM10         0.32679965  1.5753369  3.370038 25.86702  6.251683 132.59654    6\ndimension(s):\n     from to          offset  delta  refsys point\ntime    1  3 2024-10-01 CEST 1 days POSIXct    NA\ngeom    1  7              NA     NA  WGS 84  TRUE\n                                                      values\ntime                                                    NULL\ngeom POINT (12.91207 47.71819),...,POINT (12.45901 47.73125)\n```\n\n\n:::\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code  code-fold=\"true\"}\n# for some reason the aggregation step is not recognized \n# by geom_stars so I converted the stars object to a long \n# sf and used geom_sf instead\nggplot(drop_units(st_as_sf(opensense_daily, long = TRUE))) +\n  geom_sf(aes(color = temperature), size = 2) +\n  scale_color_viridis_c(\n    \"Mean temperature [Â°C]\",\n    na.value = \"grey85\", option = \"inferno\"\n  ) +\n  facet_wrap(~time, ncol = 3) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Daily mean temperature for openSenseMap stations.](index_files/figure-html/unnamed-chunk-15-1.png){width=1152}\n:::\n:::\n\n\n\nSeveral other operations can be performed using vector data cubes. Check the annotations on the margins for more content!\n\n## Converting to `{cubble}`\n\nWe can directly convert our `stars` object into a `cubble`. \nWe will use a couple of tricks to do so.\n\nA `cubble` requires a `key` which is the spatial identifier, and an `index` which is the temporal identifier. \nThe `index` is quite straightforward since we can pass the time dimension directly.\nPassing a geometry as a `key`, however, is not well supported yet, and therefore we use the trick of calling an \"id\" dimension.\nThis works because during coercion from a `stars` object `{cubble}` creates an internal id column to identify each spatial element.\nThis might change, but for now it is the workaround we can take advantage of. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopensense_cubble = opensense_stars |> \n  as_cubble(key = id, index = time)\n```\n:::\n\n\n\n::: column-margin\n\nLearn more about [cubble data structure here](https://huizezhang-sherry.github.io/cubble/index.html#how-cubble-organises-spatio-temporal-data).\n\n![Spatiotemporal data structure in `{cubble}`](https://huizezhang-sherry.github.io/cubble/reference/figures/cubble-operations.png)\n:::\n\n`{cubble}` organizes spatiotemporal data in two faces, a nested cubble represents the temporal face, and a long cubble holds the time series.\nOne can pivot back and forth from either face using the `face_spatial()` and `face_temporal()` functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopensense_cubble |> face_spatial()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ„¹ The cubble is already in the nested form\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# cubble:   key: id [7], index: time, nested form, [sf]\n# spatial:  [12.19, 47.62, 13.13, 47.87], WGS 84\n# temporal: time [dttm], temperature [[Â°C]], PM2.5 [[Âµg/mÂ³]], PM10 [[Âµg/mÂ³]]\n     id  long   lat                geom ts               \n* <int> <dbl> <dbl>         <POINT [Â°]> <list>           \n1     1  12.9  47.7 (12.91207 47.71819) <tibble [50 Ã— 4]>\n2     2  12.2  47.6 (12.18844 47.62055) <tibble [50 Ã— 4]>\n3     3  12.6  47.9 (12.64558 47.86673) <tibble [50 Ã— 4]>\n4     4  13.0  47.8 (13.03966 47.82361) <tibble [50 Ã— 4]>\n5     5  13.1  47.7 (13.12862 47.65276) <tibble [50 Ã— 4]>\n6     6  13.0  47.9  (12.97138 47.8726) <tibble [50 Ã— 4]>\n7     7  12.5  47.7 (12.45901 47.73125) <tibble [50 Ã— 4]>\n```\n\n\n:::\n\n```{.r .cell-code}\nopensense_cubble |> face_temporal()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# cubble:   key: id [7], index: time, long form\n# temporal: 2024-10-01 02:00:00 -- 2024-10-03 03:00:00 [1h], no gaps\n# spatial:  long [dbl], lat [dbl], geom [POINT [Â°]]\n      id time                temperature   PM2.5    PM10\n   <int> <dttm>                     [Â°C] [Âµg/mÂ³] [Âµg/mÂ³]\n 1     1 2024-10-01 02:00:00          NA   0.754    1.35\n 2     1 2024-10-01 03:00:00          NA   0.707    1.60\n 3     1 2024-10-01 04:00:00          NA   0.885    2.05\n 4     1 2024-10-01 05:00:00          NA   0.841    1.70\n 5     1 2024-10-01 06:00:00          NA   0.878    1.57\n 6     1 2024-10-01 07:00:00          NA   0.8      1.61\n 7     1 2024-10-01 08:00:00          NA   1.79     3.87\n 8     1 2024-10-01 09:00:00          NA   1.15     3.41\n 9     1 2024-10-01 10:00:00          NA   1.09     2.44\n10     1 2024-10-01 11:00:00          NA   0.865    2.06\n# â„¹ 340 more rows\n```\n\n\n:::\n:::\n\n\n\nOne strong point for `{cubble}` when dealing with spatiotemporal data is that there is no need any more to have an even number of observations as with an array.\n\nIf we create a `cubble` from our original opensense data, this becomes clearer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopensense |> \n  drop_na(arithmeticMean_1h) |> \n  select(-unit) |> \n  pivot_wider(names_from = phenomenon, values_from = arithmeticMean_1h) |> \n  as_cubble(key = boxName, index = time_start)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# cubble:   key: boxName [7], index: time_start, nested form, [sf]\n# spatial:  [12.19, 47.62, 13.13, 47.87], WGS 84\n# temporal: time_start [dttm], PM10 [dbl], PM2.5 [dbl], Temperatur [dbl]\n  boxName               long   lat                geom ts               \n* <chr>                <dbl> <dbl>         <POINT [Â°]> <list>           \n1 1188BRD16             12.9  47.7 (12.91207 47.71819) <tibble [50 Ã— 4]>\n2 13330108              12.2  47.6 (12.18844 47.62055) <tibble [50 Ã— 4]>\n3 BayernLab Traunstein  12.6  47.9 (12.64558 47.86673) <tibble [49 Ã— 4]>\n4 iDEAS:lab             13.0  47.8 (13.03966 47.82361) <tibble [50 Ã— 4]>\n5 Kuchl Garnei          13.1  47.7 (13.12862 47.65276) <tibble [50 Ã— 4]>\n6 Reit                  13.0  47.9  (12.97138 47.8726) <tibble [50 Ã— 4]>\n7 UWoessen              12.5  47.7 (12.45901 47.73125) <tibble [50 Ã— 4]>\n```\n\n\n:::\n:::\n\n\n\nNote the information under the `ts` column. You can now see there that the number of observations logged for each station is unequal.\n\n`cubble` has many interesting functions, but one of the highlights for me is the ability to create glyph maps. \nA glyph map allows you to understand the temporal dynamics of your data, while also considering their location in space. \n\n::: column-margin\n\n![](https://huizezhang-sherry.github.io/cubble/reference/figures/logo.svg)\n\nCheck out more about this really interesting package and follow its development here: <https://huizezhang-sherry.github.io/cubble/>\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# bring back station names\nboxnames = select(opensense, boxName) |> distinct()\nopensense_cubble = opensense_cubble |> \n  st_join(boxnames) \n# unfold the necessary data in the temporal cube to plot\nopensense_unf = opensense_cubble |> \n  face_temporal() |> \n  unfold(boxName, long, lat) |> \n  drop_units()\n# glyph plot\nggplot(opensense_unf) +\n  aes(x_major = long, x_minor = time, \n      y_major = lat, y_minor = `PM2.5`) +\n  geom_glyph_box(width = 0.08, height = 0.025) +\n  geom_glyph(aes(color = `PM2.5`), width = 0.08, height = 0.025) +\n  geom_text(\n    data = opensense_cubble,\n    aes(x = long, y = lat, label = boxName),\n    nudge_y = 0.02,\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis_c() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Glyph map of PM2.5 hourly measurements for openSenseMap stations. ](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n# VDCs for dynamic spatial data\n\nThe reason why I got so interested in VDCs was because I work often with SITS. \nA lot of the work we do at my research groups ([Risk, Hazard & Climate](https://www.plus.ac.at/geoinformatik/research/research-areas/risk-hazard-climate/?lang=en) and [EO-Analytics](https://www.plus.ac.at/geoinformatik/research/research-areas/eo-analytics/?lang=en)) is to map natural hazards to monitor their evolution using Earth observation (EO) data cubes.\nThis mapping often results in vector delineations derived from EO data.\nThey are spatio-temporal in essence but once the delineation was finished, the nice properties of data cubes.\n\nThen I realized that vector data cubes could be a potential way to organize these \"dynamic spatial data\".\nVDCs are meant to work with vector geometries but the big challenge here was that unlike weather stations, my geometries where evolving over time.\nMoreover, the way the geometries evolve is important and it is necessary to keep that complex polygon structure within the analysis. \n\nThat is how I ended up coming with the concept of summary geometries and with the decision to add the changing geometries as elements of the array.\n\n::: column-margin\nLearn more about the concept of summary geometries in the [package vignette](https://loreabad6.github.io/post/articles/post02_geomsum.html).\n:::\n\n::: {layout=\"[65,35]\" layout-valign=\"bottom\"}\n\n![](https://loreabad6.github.io/post/articles/figs/geoms-complete-post.png)\n\n![](https://loreabad6.github.io/post/articles/figs/geom_sum-post.png)\n:::\n\nAlthough relatively straightforward, setting up vector data cubes both in `{stars}` and `{cubble}` that support dynamic spatial data required the creation of new classes and methods that could easily support this *special* data structure.\n\nThat is how [`{post}`](https://loreabad6.github.io/post/) started. It stands for Polygon changes in Space and Time and its main goal is \"to provide array and tabular vector data cube structures to organise and analyse polygons that change their shape in space and time\".\n\n::: column-margin\n\n![A new package is born! ğŸ“¦](https://loreabad6.github.io/post/reference/figures/hexlogo.png)\n:::\n\nThe package is still work in progress but it already has a website and some basic documentation on its main functionalities. \nThe next step for me is to work on raster to vector data cube aggregations considering the dynamic nature of the data. \nYou can follow documentation of [that particular issue here](https://github.com/loreabad6/post/issues/1).\n\nI will be writing more posts on how I use `{post}` in my work and of the new features I am planning to implement.\nI hope this gives you a bit of curiosity and if you want, test the package already and let me know what you think!\n\nThanks for reading this post and attending the talk if you did! Until next time!\n\n\n<details>\n\n<summary>Session info</summary>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.4.1 (2024-06-14 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language ENG\n collate  English_Austria.utf8\n ctype    English_Austria.utf8\n tz       Europe/Vienna\n date     2024-10-15\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package     * version    date (UTC) lib source\n abind       * 1.4-5      2016-07-21 [1] CRAN (R 4.2.0)\n anytime       0.3.9      2020-08-27 [1] CRAN (R 4.2.3)\n class         7.3-22     2023-05-03 [1] CRAN (R 4.3.1)\n classInt      0.4-10     2023-09-05 [1] CRAN (R 4.3.1)\n cli           3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n colorspace    2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n cubble      * 1.0.0      2024-09-16 [1] local\n DBI           1.2.3      2024-06-02 [1] CRAN (R 4.4.0)\n digest        0.6.35     2024-03-11 [1] CRAN (R 4.4.0)\n dplyr       * 1.1.4      2023-11-17 [1] CRAN (R 4.3.2)\n e1071         1.7-14     2023-12-06 [1] CRAN (R 4.3.2)\n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.2.3)\n evaluate      0.23       2023-11-01 [1] CRAN (R 4.3.2)\n fansi         1.0.6      2023-12-08 [1] CRAN (R 4.3.2)\n farver        2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n fastmap       1.1.1      2023-02-24 [1] CRAN (R 4.2.3)\n forcats     * 1.0.0      2023-01-29 [1] CRAN (R 4.2.3)\n generics      0.1.3      2022-07-05 [1] CRAN (R 4.2.3)\n ggplot2     * 3.5.1      2024-04-23 [1] CRAN (R 4.4.0)\n glue        * 1.7.0      2024-01-09 [1] CRAN (R 4.3.2)\n gtable        0.3.5      2024-04-22 [1] CRAN (R 4.4.0)\n here        * 1.0.1      2020-12-13 [1] CRAN (R 4.2.3)\n hms           1.1.3      2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.8.1    2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets   1.6.4      2023-12-06 [1] CRAN (R 4.3.2)\n jsonlite      1.8.8      2023-12-04 [1] CRAN (R 4.3.2)\n KernSmooth    2.23-22    2023-07-10 [1] CRAN (R 4.3.1)\n knitr         1.46       2024-04-06 [1] CRAN (R 4.4.0)\n labeling      0.4.3      2023-08-29 [1] CRAN (R 4.3.1)\n lifecycle     1.0.4      2023-11-07 [1] CRAN (R 4.3.2)\n lubridate   * 1.9.3      2023-09-27 [1] CRAN (R 4.3.2)\n magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.1.3)\n munsell       0.5.1      2024-04-01 [1] CRAN (R 4.4.0)\n ncdf4         1.23       2024-08-17 [1] CRAN (R 4.4.1)\n pillar        1.9.0      2023-03-22 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.2.3)\n post        * 0.0.0.9000 2024-10-09 [1] local\n proxy         0.4-27     2022-06-09 [1] CRAN (R 4.2.3)\n purrr       * 1.0.2      2023-08-10 [1] CRAN (R 4.3.1)\n R6            2.5.1      2021-08-19 [1] CRAN (R 4.2.3)\n Rcpp          1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n readr       * 2.1.5      2024-01-10 [1] CRAN (R 4.4.0)\n rlang         1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n rmarkdown     2.28       2024-08-17 [1] CRAN (R 4.4.1)\n rprojroot     2.0.4      2023-11-05 [1] CRAN (R 4.4.0)\n rstudioapi    0.16.0     2024-03-24 [1] CRAN (R 4.4.0)\n s2            1.1.7      2024-07-17 [1] CRAN (R 4.4.1)\n scales        1.3.0      2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.4.0)\n sf          * 1.0-18     2024-10-09 [1] Github (r-spatial/sf@6f247a5)\n stars       * 0.6-7      2024-09-05 [1] Github (r-spatial/stars@ec1f849)\n stringi       1.8.4      2024-05-06 [1] CRAN (R 4.4.0)\n stringr     * 1.5.1      2023-11-14 [1] CRAN (R 4.3.2)\n tibble      * 3.2.1      2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.1      2024-01-24 [1] CRAN (R 4.3.2)\n tidyselect    1.2.1      2024-03-11 [1] CRAN (R 4.4.0)\n tidyverse   * 2.0.0      2023-02-22 [1] CRAN (R 4.2.3)\n timechange    0.3.0      2024-01-18 [1] CRAN (R 4.3.2)\n tsibble       1.1.5      2024-06-27 [1] CRAN (R 4.4.1)\n tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.3.1)\n units       * 0.8-5      2023-11-28 [1] CRAN (R 4.4.1)\n utf8          1.2.4      2023-10-22 [1] CRAN (R 4.3.2)\n vctrs         0.6.5      2023-12-01 [1] CRAN (R 4.3.2)\n viridisLite   0.4.2      2023-05-02 [1] CRAN (R 4.3.1)\n withr         3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n wk            0.9.3      2024-09-06 [1] CRAN (R 4.4.1)\n xfun          0.43       2024-03-25 [1] CRAN (R 4.4.0)\n yaml          2.3.8      2023-12-11 [1] CRAN (R 4.3.2)\n\n [1] C:/Users/b1066081/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.1/library\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n```\n\n\n:::\n:::\n\n\n</details>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}