---
title: "Let's cube those geoms!"
description: |
  Vector data cubes for multidimensional data.
date: 14/10/2024
date-modified: last-modified
# image: figs/thumbnail.png
categories:
  - r-spatial
  - rstats
bibliography: https://api.citedrive.com/bib/618133a2-4856-443b-b778-a72d316eb599/references.bib?x=eyJpZCI6ICI2MTgxMzNhMi00ODU2LTQ0M2ItYjc3OC1hNzJkMzE2ZWI1OTkiLCAidXNlciI6ICIyNTY4IiwgInNpZ25hdHVyZSI6ICI4MWExNDU3ODE3NWUxMzFiZDI1NDliOTg2NmQzMWJjNzU4OWI4YjI3MDUxOTU5NDg3MDM5MzFlYmQwMDY0OThhIn0=/bibliography.bib
draft: true
editor_options: 
  chunk_output_type: console
---

# BEGIN seminar

If you are here, it is very likely you attended the [BEGIN seminar from the University of St. Andrews](https://begin.wp.st-andrews.ac.uk/activities-events/) today. Thank you for stopping by! üíê

I was invited by Dr. Fernando Benitez (thank you so much for considering me!) to give a talk on a topic I am working on, so I decided to [talk about vector data cubes](https://begin.wp.st-andrews.ac.uk/2024/09/26/begin-seminar-dr-lorena-cristina-abad-crespo/) üòä.
This post compliments the talk by writing out what I discussed during the seminar and by providing the code to reproduce some of the work I showed. 

Here are the slides of my talk today:

<iframe width="580" height="400" src="https://loreabad6.github.io/slides/begin-vdcs.html" frameborder="0">
<p>Your browser does not support iframes.</p>
</iframe>

And over here I will load some of the libraries I use for the next sections.

```{r}
#| label: setup
#| warning: false
#| message: false
library(cubble)
library(glue)
library(here)
if(!require(post)) remotes::install_github("loreabad6/post")
library(post)
library(sf)
library(stars)
library(tidyverse)
library(units)
```


# Data cubes

Data cubes are ideal data structures for multidimensional data. Think of data with spatiotemporal properties such as satellite image time series (SITS). If you work with SITS you might already be familiar with data cubes, as Earth system data cubes have received quite some attention lately [@Mahecha2020].

::: column-margin
![Earth system data cube \| Source: @Mahecha2020](https://esd.copernicus.org/articles/11/201/2020/esd-11-201-2020-f03-web.png)
:::

Tools and initiatives like [Google Earth Engine](https://earthengine.google.com/), [OpenEO](https://openeo.org/) and [Digital Earth Africa](https://www.digitalearthafrica.org/) are using data cubes as their core for SITS analysis.

Organizing SITS in data cubes is intuitive and logical given that satellite images have consistent X/Y coordinates per pixel, and any reacquisition over time keeps the same X/Y coordinates. Here we already identify three dimensions: `X`, `Y` and `time`. Satellite images also have bands and we can then consider those as a fourth dimension.

![4D data cube \| Source: @Pebesma_2023](figs/fig-cube4d-1.png)

::: column-margin
I wrote a post almost a year ago on how to create [on-demand data cubes with STAC with R and Python](https://loreabad6.github.io/posts/2023-10-17-stac-across-languages/).
:::

# Vector data cubes

To quote @Pebesma_2023 at the very start of the Data Cubes Chapter:

> ‚ÄúData cubes arise naturally when we observe properties of a set of geometries repeatedly over time‚Äù.

Clearly satellite images for this description, being X and Y said ‚Äúgeometry‚Äù. But how other type of data like meteorological station data? They are also located in a X and Y location on Earth, represented by a `POINT` geometry. Stations collect data at constant time intervals, giving them a temporal dimension. Further, they can collect data on different type of phenomena, such as temperature, humidity, etc. So in essence, station data is multidimensional data that can be represented as a data cube. In this case, a *vector data cube* (VDCs).

![Vector data cube \| Source: @Pebesma_2023](figs/fig-aqdc-1.png)

::: column-margin
If you are familiar with databases, you will find that the concept of a vector data cube aligns greatly with an [OLAP cube](https://en.wikipedia.org/wiki/OLAP_cube) model.
:::

Another ‚Äúnaturally occurring‚Äù VDC case is when data from a raster or Earth system data cube is aggregated onto vector geometries. This vector geometries can be for example, political boundaries. The [`{stars}`](https://r-spatial.github.io/stars/index.html) package has a great example of aggregation of precipitation data over the North Carolina counties [right here](https://r-spatial.github.io/stars/index.html#raster-and-vector-time-series-analysis-example). This is a sneak peek:

```{r}
#| echo: true
#| code-fold: true
#| layout-ncol: 2
#| column: page
#| fig-width: 8
#| fig-height: 5
#| fig-cap: 
#|   - "Precipitation raster data cube"
#|   - "Precipitation vector data cube aggregated by NC county"
# Load precipitation data
precipitation = system.file("nc/test_stageiv_xyt.nc", package = "stars") |>
    read_stars()
# Load North Carolina data
nc = system.file("gpkg/nc.gpkg", package = "sf") |> 
  read_sf() |> 
  # Transform CRS to match precipitation data cube
  st_transform(st_crs(precipitation))
# Aggregate precipitation data over NC counties
precipitation_nc = aggregate(precipitation, by = nc, FUN = max)
# Plot raster data cube
ggplot() +
  geom_stars(data = precipitation[,,,1:9]) +
  scale_fill_viridis_c("Precipitation [kg/m^2]") +
  geom_sf(data = nc, fill = NA, color = "red") +
  facet_wrap("time", ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")
# Plot vector data cube
ggplot() +
  geom_stars(
    data = precipitation_nc[,,1:9]
  ) +
  geom_sf(data = nc, fill = NA, color = "white") +
  scale_fill_viridis_c(
    "Precipitation [kg/m^2]",
    na.value = "grey85"
  ) +
  facet_wrap(~time, nrow = 3) +
  theme_bw() +
  theme(legend.position = "bottom")
```

# Exploring VDCs with R

If you do a search for ‚Äúvector data cubes R‚Äù, it is very likely that your first result is the `{stars}` package. As its name says, `{stars}` supports ‚Äúspatiotemporal **arrays**, rasters, and vector data cubes‚Äù.

::: column-margin
`{stars}` pioneered vector data cube implementations in current spatial data science languages. Further efforts in Python with the package [`{xvec}`](https://xvec.readthedocs.io/en/stable/) are inspired by initial development in `{stars}`.
:::

`{stars}` takes advantage of base R‚Äôs support for matrices and arrays to implement common data cube operations such as slicing (filtering), reducing (aggregating), and applying functions to dimensions.

Typically, data cubes are represented as multidimensional arrays given their capacity to support exactly that, multiple dimensions.
However, when only looking at spatio-temporal data, I also use the [`{cubble}`](https://huizezhang-sherry.github.io/cubble/) package that implements "a vector spatiotemporal data structure for data analysis" [@Zhang_2022].

`{stars}` and `{cubble}` interact very well when the data cube consists of space and time dimensions. 

To showcase this interaction I will make use of some data from the [openSenseMap](https://opensensemap.org/) project and build what I call array and tabular data cubes.

::: column-margin
[openSenseMap.org](https://opensensemap.org/) has the vision to become *"the Wikipedia for environmental data in order to awaken and promote education, environmental and climate protection, enthusiasm for STEM, citizen science, open data and open source."*

You can [donate to their cause here](https://opensensemap.org/donate).
:::

## Fetching the data

The openSenseMap project provides an API to query the information from the sensors shared on their website. 

I queried the API to obtain some data from sensors close to the city where I am based, Salzburg.
You can unfold the code below to see how I queried the API and pre-processed the data so that you know how to do this for other areas.
However, I saved the result of the querying and pre-processing to avoid exhausting the API.
The data is corpyrighted as ¬© senseBox 2014 - 2020.

```{r}
#| eval: false
#| code-fold: true

## parameters
bboxsbg = glue_collapse(c(12,47.5,13.5,47.9), sep = ",")
from = "2024-10-01T00:00:00Z"
to = "2024-10-03T00:00:00Z"
op = "arithmeticMean"
win = "1h"
format = "tidy"
cols = glue_collapse(c("boxId","boxName","lat","lon","phenomenon","unit"), sep = ",")
## api query
query = function(phenomenon) {
  glue("https://api.opensensemap.org/statistics/descriptive?bbox={bboxsbg}&from-date={from}&to-date={to}&phenomenon={phenomenon}&operation={op}&columns={cols}&window={win}&format={format}")
}
## fetch data
temperature = read_csv(query("Temperatur"))
pm10 = read_csv(query("PM10"))
pm25 = read_csv(query("PM2.5"))
## check if data is complete, if not, complete cases
dim(temperature)
dim(pm10)
dim(pm25)
temperature_comp = temperature |> 
  complete(
    nesting(boxName, lon, lat), time_start,
    nesting(phenomenon, unit)
)
# merge data into one file and complete cases for all stations
opensense = bind_rows(temperature_comp, pm10, pm25) |> 
  select(-sensorId, -boxId) |> 
  complete(
    nesting(boxName, lon, lat, time_start),
    nesting(phenomenon, unit)
  ) |> 
  st_as_sf(coords = c("lon", "lat")) |> 
  st_set_crs(4326)
```

To read in the saved data, we can use the `{sf}` package.

```{r}
(opensense = read_sf("data/opensensemap.gpkg"))
```

As you can notice, the data has several data duplicates, since it is presented in a long format. 
In this way each row contains the station name (`boxName`), the timestamp (`time_start`) and the parameter measured (`phenomenon`). 
Each station also has a `POINT` geometry and each measurement has its corresponding unit. 

The measured values (`arithmeticMean_1h`) are the only changing variables. 
This value, as can be seen on the query code, is the arithmetic mean per hour of the observations for temperature, PM2.5 and PM10 for the sensors in the bounding box given.
Not every sensor measures all the values and therefore there are `NA`s within the data. 

## Creating a VDC with `{stars}`

Although creating a raster data cube from SITS data is pretty straightforward in `{stars}`, creating a VDC requires a bit more pre-processing.

However, when creating a VDC from scratch we get to understand better how [`stars` objects are structured](https://r-spatial.github.io/stars/articles/stars4.html).

> "`{stars}` provides infrastructure for *data cubes*, array data with labeled dimensions".

So, we start with the *dimensions*.

> "A `dimensions` object is a named list of dimension elements, each describing the semantics a dimension of the data arrays (space, time, type etc)."

First, we identify the dimensions in the `opensense` object.
We have a geometry (`geom`) that identifies and locates each station in space and we have a timestamp (`time_start`) that marks the temporal dimension.
The `point` parameter indicates if the dimension has point support (`point = TRUE` in the case of our `geom`) or if it is a period (`point = FALSE` in the case of our `time_start`).

```{r}
(d = st_dimensions(
  geom = st_sfc(unique(opensense$geom), crs = st_crs(opensense)),
  time = unique(opensense$time_start),
  point = c(TRUE, FALSE)
))
```

<!-- add explanation on the dimensions object -->

Now, let's focus on the *array data*. 

> "stars objects consist of a (possibly empty) named list of arrays, each having named dimensions (`dim`) attribute".

To create an array in R we need to establish the maximal indices in each dimension. 
As we did above when building the `dimensions` object, we create now a vector with the dimensions in which we will arrange the array data. 

```{r}
dims = c(
  length(unique(opensense$boxName)),
  length(unique(opensense$time_start))
)
```

The order in which the data is assigned to each element in an array is important. 
Normally, arrays are assigned in a per column basis. 
For this, we order our data first by the last set dimension (time) and then by the first one (station geom).

```{r}
opensense = opensense[order(opensense[["time_start"]],opensense[["boxName"]]), ]
```

And now we create the arrays to pass onto the `stars` object during creation. 
We create one array per parameter.
For that we filter the data to the corresponding `phenomenon`.
Then we extract the measured values and pass them onto the `array()` function, assigning the `dims` object created above. 
Finally, we pass the units to the array for each parameter.

::: column-margin
Instead of creating the three arrays, we can also create a third dimension for the parameters.
:::

```{r}
# Create temperature array
temp = filter(opensense, phenomenon == "Temperatur")
temp_arr = array(temp$arithmeticMean_1h, dim = dims)
units(temp_arr) = as_units(unique(temp$unit))
# Create PM2.5 array
pm25 = filter(opensense, phenomenon == "PM2.5")
pm25_arr = array(pm25$arithmeticMean_1h, dim = dims)
units(pm25_arr) = as_units(unique(pm25$unit))
# Create PM10 array
pm10 = filter(opensense, phenomenon == "PM10")
pm10_arr = array(pm10$arithmeticMean_1h, dim = dims)
units(pm10_arr) = as_units(unique(pm10$unit))
```

And with that, we can create our VDC using `{stars}`.

```{r}
(opensense_stars = st_as_stars(
  temperature = temp_arr,
  `PM2.5` = pm25_arr,
  PM10 = pm10_arr,
  d
))
```


```{r}
#| column: page
#| fig-width: 12
#| fig-height: 4
#| fig-cap: "PM10 hourly measurements for openSenseMap stations."
plot(opensense_stars["PM10",,1:9], pch = 16)
```

```{r}
opensense_stars |> 
  drop_units() |> 
  merge(name = "parameter")
```

```{r}
opensense_daily = opensense_stars |> 
  aggregate("1 day", mean, na.rm = TRUE)
```

```{r}
#| column: page
#| code-fold: true
#| fig-width: 12
#| fig-height: 2.5
#| fig-cap: "Daily mean temperature for openSenseMap stations."
# for some reason the aggregation step is not recognized 
# by geom_stars so I converted the stars object to a long 
# sf and used geom_sf instead
ggplot(drop_units(st_as_sf(opensense_daily, long = TRUE))) +
  geom_sf(aes(color = temperature), size = 2) +
  scale_color_viridis_c(
    "Mean temperature [¬∞C]",
    na.value = "grey85", option = "inferno"
  ) +
  facet_wrap(~time, ncol = 3) +
  theme_bw()
```

## Converting to `{cubble}`

```{r}
opensense_cubble = opensense_stars |> 
  as_cubble(key = id, index = time)
```

```{r}
opensense_cubble |> face_spatial()
opensense_cubble |> face_temporal()
```

```{r}
#| code-fold: true
#| warning: false
#| fig-cap: "Glyph map of PM2.5 hourly measurements for openSenseMap stations. "
# bring back station names
boxnames = select(opensense, boxName) |> distinct()
opensense_cubble = opensense_cubble |> 
  st_join(boxnames) 
# unfold the necessary data in the temporal cube to plot
opensense_unf = opensense_cubble |> 
  face_temporal() |> 
  unfold(boxName, long, lat) |> 
  drop_units()
# glyph plot
ggplot(opensense_unf) +
  aes(x_major = long, x_minor = time, 
      y_major = lat, y_minor = `PM2.5`) +
  geom_glyph_box(width = 0.08, height = 0.025) +
  geom_glyph(aes(color = `PM2.5`), width = 0.08, height = 0.025) +
  geom_text(
    data = opensense_cubble,
    aes(x = long, y = lat, label = boxName),
    nudge_y = 0.02,
    inherit.aes = FALSE
  ) +
  scale_color_viridis_c() +
  theme_bw()
```

# VDCs for dynamic spatial data

